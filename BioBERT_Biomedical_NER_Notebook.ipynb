{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8557e829",
   "metadata": {},
   "source": [
    "# Biomedical Named Entity Recognition with BioBERT\n",
    "\n",
    "**Ready-to-run Jupyter/Colab notebook** — train, evaluate, and run inference for biomedical NER using `dmis-lab/biobert-base-cased-v1.1` (HuggingFace).\n",
    "\n",
    "**Notebook contents**\n",
    "1. Install & setup\n",
    "2. Load dataset (BC5CDR example via `datasets`)\n",
    "3. Preprocessing & token-label alignment (BIO)\n",
    "4. Model setup (`AutoModelForTokenClassification`)\n",
    "5. Training with `Trainer`\n",
    "6. Evaluation with `seqeval`\n",
    "7. Inference helper & demo\n",
    "8. Tips for improvements and extensions\n",
    "\n",
    "**Notes**\n",
    "- This notebook expects an environment with internet access (to download models/datasets). For Colab, select a GPU runtime.\n",
    "- If you're behind a firewall, download datasets and models manually and adjust paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2e13e81122cbea9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T16:23:35.926585Z",
     "start_time": "2025-11-15T16:23:35.804354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./env/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: seqeval in ./env/lib/python3.13/site-packages (1.2.2)\n",
      "Requirement already satisfied: accelerate in ./env/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: evaluate in ./env/lib/python3.13/site-packages (0.4.6)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./env/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.13/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./env/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in ./env/lib/python3.13/site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./env/lib/python3.13/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./env/lib/python3.13/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: dill in ./env/lib/python3.13/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.13/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: xxhash in ./env/lib/python3.13/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in ./env/lib/python3.13/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./env/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./env/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./env/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./env/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./env/lib/python3.13/site-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.13/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.13/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.13/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers seqeval accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0a38ac46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:23:32.588510Z",
     "start_time": "2025-11-15T14:23:32.575624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.57.1\n",
      "datasets 3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "import transformers, datasets\n",
    "print('transformers', transformers.__version__)\n",
    "print('datasets', datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6fe30052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (BC5CDR for chemicals/diseases) via HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "tner_dataset = load_dataset('tner/bc5cdr')\n",
    "print(tner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "89710e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['tokens', 'tags'])\n",
      "tokens sample: ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.']\n",
      "tags sample: [1, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Inspect an example\n",
    "example = tner_dataset['train'][0]\n",
    "print('keys:', example.keys())\n",
    "if 'tokens' in example:\n",
    "    print('tokens sample:', example['tokens'][:40])\n",
    "if 'tags' in example:\n",
    "    print('tags sample:', example['tags'][:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f19859",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f0dd6308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5228\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5865\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing: tokenize and align labels (BIO scheme)\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Utility to align labels for tokenized inputs\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, truncation=True, padding='max_length', max_length=256)\n",
    "    all_labels = examples['tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        new_labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Map over dataset\n",
    "tokenized_datasets = tner_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=tner_dataset['train'].column_names)\n",
    "tokenized_datasets.set_format(type='torch')\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6f852",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "80711160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_label_map(dataset):\n",
    "    unique_ids = sorted(\n",
    "        set(tag for ex in dataset[\"train\"][\"tags\"] for tag in ex)\n",
    "    )\n",
    "\n",
    "    # If 0 is present → assume \"O\"\n",
    "    label_names = [\"O\"] if 0 in unique_ids else []\n",
    "\n",
    "    # Find entity spans\n",
    "    spans = []\n",
    "    for tokens, tags in zip(dataset[\"train\"][\"tokens\"], dataset[\"train\"][\"tags\"]):\n",
    "        current = []\n",
    "        for tok, tag in zip(tokens, tags):\n",
    "            if tag != 0:\n",
    "                current.append(tok)\n",
    "            elif current:\n",
    "                spans.append(tuple(current))\n",
    "                current = []\n",
    "        if current:\n",
    "            spans.append(tuple(current))\n",
    "\n",
    "    # Unique span samples → unique entity categories\n",
    "    entity_types = [\"Chemical\", \"Disease\"]\n",
    "\n",
    "    for ent in entity_types:\n",
    "        label_names.append(f\"B-{ent}\")\n",
    "        label_names.append(f\"I-{ent}\")\n",
    "\n",
    "    return label_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "84946a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-Chemical', 'I-Chemical', 'B-Disease', 'I-Disease']\n",
      "{0: 'O', 1: 'B-Chemical', 2: 'I-Chemical', 3: 'B-Disease', 4: 'I-Disease'}\n",
      "{'O': 0, 'B-Chemical': 1, 'I-Chemical': 2, 'B-Disease': 3, 'I-Disease': 4}\n"
     ]
    }
   ],
   "source": [
    "# Get label list from dataset feature (adjust if different)\n",
    "label_names = infer_label_map(tner_dataset)\n",
    "\n",
    "print(label_names)\n",
    "\n",
    "id_to_label = {i: label_names[i] for i in range(len(label_names))}\n",
    "label_to_id = {label_names[i]: i for i in range(len(label_names))}\n",
    "\n",
    "print(id_to_label)\n",
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c2f80903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model setup for token classification\n",
    "from transformers import BertConfig, BertForTokenClassification\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e1056",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6bd0c604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1635' max='1635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1635/1635 42:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.089167</td>\n",
       "      <td>0.853340</td>\n",
       "      <td>0.846017</td>\n",
       "      <td>0.849663</td>\n",
       "      <td>0.970309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.092946</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.860139</td>\n",
       "      <td>0.971558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.104850</td>\n",
       "      <td>0.857166</td>\n",
       "      <td>0.874514</td>\n",
       "      <td>0.865753</td>\n",
       "      <td>0.972780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.117621</td>\n",
       "      <td>0.858666</td>\n",
       "      <td>0.880424</td>\n",
       "      <td>0.869409</td>\n",
       "      <td>0.973211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.124546</td>\n",
       "      <td>0.856929</td>\n",
       "      <td>0.880586</td>\n",
       "      <td>0.868597</td>\n",
       "      <td>0.972908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadarpur/Desktop/biomedical-named-entity-recognition/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hadarpur/Desktop/biomedical-named-entity-recognition/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hadarpur/Desktop/biomedical-named-entity-recognition/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hadarpur/Desktop/biomedical-named-entity-recognition/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1635, training_loss=0.052293134391854665, metrics={'train_runtime': 2577.2463, 'train_samples_per_second': 10.143, 'train_steps_per_second': 0.634, 'total_flos': 3415241238988800.0, 'train_loss': 0.052293134391854665, 'epoch': 5.0})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with HuggingFace Trainer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Metrics - using seqeval for entity-level metrics\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        filtered_preds = []\n",
    "        filtered_labels = []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:               # <--- ignore padding tokens\n",
    "                filtered_preds.append(id_to_label[int(p_i)])\n",
    "                filtered_labels.append(id_to_label[int(l_i)])\n",
    "        true_predictions.append(filtered_preds)\n",
    "        true_labels.append(filtered_labels)\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    overall = {\n",
    "        'overall_precision': results['overall_precision'],\n",
    "        'overall_recall': results['overall_recall'],\n",
    "        'overall_f1': results['overall_f1'],\n",
    "        'overall_accuracy': results['overall_accuracy']\n",
    "    }\n",
    "    return overall\n",
    "\n",
    "batch_size = 16\n",
    "args = TrainingArguments(\n",
    "    output_dir = 'biobert-ner-run',\n",
    "    eval_strategy = 'epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Uncomment to train (requires GPU for reasonable speed)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0998ae8",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "32dc6c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-Chemical', 'score': np.float32(0.99944955), 'index': 1, 'word': 'As', 'start': 0, 'end': 2}, {'entity': 'B-Chemical', 'score': np.float32(0.8595811), 'index': 2, 'word': '##pi', 'start': 2, 'end': 4}, {'entity': 'I-Disease', 'score': np.float32(0.8810123), 'index': 3, 'word': '##rin', 'start': 4, 'end': 7}, {'entity': 'I-Chemical', 'score': np.float32(0.9987203), 'index': 8, 'word': 'pain', 'start': 31, 'end': 35}, {'entity': 'I-Chemical', 'score': np.float32(0.99668103), 'index': 14, 'word': 'r', 'start': 70, 'end': 71}, {'entity': 'B-Disease', 'score': np.float32(0.8686689), 'index': 15, 'word': '##he', 'start': 71, 'end': 73}, {'entity': 'B-Disease', 'score': np.float32(0.9250321), 'index': 16, 'word': '##uma', 'start': 73, 'end': 76}, {'entity': 'B-Disease', 'score': np.float32(0.99834096), 'index': 17, 'word': '##to', 'start': 76, 'end': 78}, {'entity': 'B-Disease', 'score': np.float32(0.99886876), 'index': 18, 'word': '##id', 'start': 78, 'end': 80}, {'entity': 'B-Disease', 'score': np.float32(0.99936694), 'index': 19, 'word': 'art', 'start': 81, 'end': 84}, {'entity': 'B-Disease', 'score': np.float32(0.9987368), 'index': 20, 'word': '##hr', 'start': 84, 'end': 86}, {'entity': 'B-Disease', 'score': np.float32(0.99956757), 'index': 21, 'word': '##itis', 'start': 86, 'end': 90}]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate (run after training or with a pre-trained checkpoint)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pipeline for token-classification (fast option)\n",
    "nlp = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    ignore_subwords=True\n",
    ")\n",
    "\n",
    "# Demo (replace with an actual abstract)\n",
    "demo_text = \"Aspirin is frequently used for pain relief in patients suffering from rheumatoid arthritis.\"\n",
    "raw = nlp(demo_text)\n",
    "\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f8931759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Type  | Text                           | Score  | Start-End\n",
      "-----------------------------------------------------------------\n",
      "B-Chemical   | Aspirin                        | 0.999  | 0-7\n",
      "I-Chemical   | pain                           | 0.999  | 31-35\n",
      "I-Chemical   | rheumatoid                     | 0.999  | 70-80\n",
      "B-Disease    | arthritis                      | 1.0    | 81-90\n"
     ]
    }
   ],
   "source": [
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        w = ent['word']\n",
    "        if w.startswith('##') and merged:\n",
    "            # attach to previous token (remove '##'), extend end, update score\n",
    "            prev = merged[-1]\n",
    "            prev['word'] += w[2:]\n",
    "            prev['end'] = ent['end']\n",
    "            prev['score'] = max(prev['score'], float(ent['score']))\n",
    "        else:\n",
    "            # normalize numpy floats to Python float\n",
    "            ent['score'] = float(ent['score'])\n",
    "            merged.append(dict(ent))\n",
    "    return merged\n",
    "\n",
    "def display_entities(entities):\n",
    "    print(f\"{'Entity Type':<12} | {'Text':<30} | {'Score':<6} | Start-End\")\n",
    "    print(\"-\" * 65)\n",
    "    for e in entities:\n",
    "        entity_type = e['entity']\n",
    "        word = e['word']\n",
    "        score = round(float(e['score']), 3)  # round to 3 decimals\n",
    "        start, end = e['start'], e['end']\n",
    "        print(f\"{entity_type:<12} | {word:<30} | {score:<6} | {start}-{end}\")\n",
    "\n",
    "ents_merged = merge_subwords(raw)  # from previous step\n",
    "display_entities(ents_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0140bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-Chemical', 'text': 'Aspi'}, {'entity': 'I-Disease', 'text': 'rin'}, {'entity': 'O', 'text': 'isfrequentlyusedfor'}, {'entity': 'I-Chemical', 'text': 'pain'}, {'entity': 'O', 'text': 'reliefinpatientssufferingfrom'}, {'entity': 'I-Chemical', 'text': 'r'}, {'entity': 'B-Disease', 'text': 'heumatoidarthritis'}, {'entity': 'O', 'text': '.'}]\n"
     ]
    }
   ],
   "source": [
    "# Inference helper (function) for raw text -> BIO spans\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_entities_from_text(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        preds = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "\n",
    "    input_ids = tokens[\"input_ids\"].squeeze().tolist()\n",
    "    id2label = model.config.id2label\n",
    "\n",
    "    entities = []\n",
    "    current_text = \"\"\n",
    "    current_label = None\n",
    "\n",
    "    for token_id, pred in zip(input_ids, preds):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        label = id2label[pred]\n",
    "\n",
    "        # skip special tokens\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "\n",
    "        # remove subword markers\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "\n",
    "        if label == current_label:\n",
    "            # continuation\n",
    "            current_text += clean_token\n",
    "        else:\n",
    "            # start a new entity\n",
    "            if current_label is not None:\n",
    "                entities.append({\"entity\": current_label, \"text\": current_text})\n",
    "            \n",
    "            # start the next one\n",
    "            current_label = label\n",
    "            current_text = clean_token\n",
    "\n",
    "    # append last entity\n",
    "    if current_label is not None:\n",
    "        entities.append({\"entity\": current_label, \"text\": current_text})\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "print(get_entities_from_text(demo_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6cf5c",
   "metadata": {},
   "source": [
    "## 8) Tips, improvements and next steps\n",
    "\n",
    "- **Longer training**: increase epochs to 4-6 and use LR warmup.\n",
    "- **Domain adaptation**: further pretrain on in-domain corpus (PubMed subset) using MLM before fine-tuning.\n",
    "- **Label scheme**: consider BIOES for improved boundary detection.\n",
    "- **Data augmentation**: back-translation, mention replacement.\n",
    "- **Evaluation**: produce per-entity confusion matrices and error analysis.\n",
    "- **Deploy**: use FastAPI + Docker or a Gradio demo for quick sharing.\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can also produce:\n",
    "- A runnable Colab link (I can adapt the notebook for immediate Colab paste),\n",
    "- A minimal Streamlit/Gradio demo app, or\n",
    "- A LaTeX-ready project report template summarizing experiments and results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
