{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8557e829",
   "metadata": {},
   "source": [
    "# Biomedical Named Entity Recognition with BioBERT\n",
    "\n",
    "**Ready-to-run Jupyter/Colab notebook** â€” train, evaluate, and run inference for biomedical NER using `dmis-lab/biobert-base-cased-v1.1` (HuggingFace).\n",
    "\n",
    "**Notebook contents**\n",
    "1. Install & setup\n",
    "2. Load dataset (BC5CDR example via `datasets`)\n",
    "3. Preprocessing & token-label alignment (BIO)\n",
    "4. Model setup (`BertCRFForNER`)\n",
    "5. Training with `Trainer`\n",
    "6. Evaluation\n",
    "7. Inference helper & demo\n",
    "\n",
    "**Notes**\n",
    "- This notebook expects an environment with internet access (to download models/datasets). For Colab, select a GPU runtime.\n",
    "- If you're behind a firewall, download datasets and models manually and adjust paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e13e81122cbea9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T16:23:35.926585Z",
     "start_time": "2025-11-15T16:23:35.804354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.44 in ./env/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets>=2.21 in ./env/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: seqeval in ./env/lib/python3.13/site-packages (1.2.2)\n",
      "Requirement already satisfied: torch in ./env/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.13/site-packages (from transformers>=4.44) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.13/site-packages (from transformers>=4.44) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.13/site-packages (from transformers>=4.44) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./env/lib/python3.13/site-packages (from datasets>=2.21) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./env/lib/python3.13/site-packages (from datasets>=2.21) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.13/site-packages (from datasets>=2.21) (2.3.3)\n",
      "Requirement already satisfied: xxhash in ./env/lib/python3.13/site-packages (from datasets>=2.21) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./env/lib/python3.13/site-packages (from datasets>=2.21) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./env/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (3.13.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in ./env/lib/python3.13/site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./env/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./env/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./env/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./env/lib/python3.13/site-packages (from requests->transformers>=4.44) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.13/site-packages (from requests->transformers>=4.44) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.13/site-packages (from requests->transformers>=4.44) (2025.11.12)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./env/lib/python3.13/site-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.13/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.13/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.13/site-packages (from pandas->datasets>=2.21) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.13/site-packages (from pandas->datasets>=2.21) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.13/site-packages (from pandas->datasets>=2.21) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers>=4.44\" \"datasets>=2.21\" \"seqeval\" \"torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7150e94",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb72066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import transformers, datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a38ac46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:23:32.588510Z",
     "start_time": "2025-11-15T14:23:32.575624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.57.1\n",
      "datasets 3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "print('transformers', transformers.__version__)\n",
    "print('datasets', datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb6403",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf671f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "DATASET_NAME = \"tner/bc5cdr\"  # pre-split, tokenized BC5CDR with tags:contentReference[oaicite:1]{index=1}\n",
    "\n",
    "label_list = [\"O\", \"B-Chemical\", \"B-Disease\", \"I-Disease\", \"I-Chemical\"]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b950d",
   "metadata": {},
   "source": [
    "## Load dataset and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe30052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: tner/bc5cdr\n",
      "Splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (BC5CDR for chemicals/diseases) via HuggingFace datasets\n",
    "print(\"Loading dataset:\", DATASET_NAME)\n",
    "tner_dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "print(\"Splits:\", tner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89710e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['tokens', 'tags'])\n",
      "tokens sample: ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.']\n",
      "tags sample: [1, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Inspect an example\n",
    "example = tner_dataset['train'][0]\n",
    "print('keys:', example.keys())\n",
    "if 'tokens' in example:\n",
    "    print('tokens sample:', example['tokens'][:40])\n",
    "if 'tags' in example:\n",
    "    print('tags sample:', example['tags'][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0dd6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: dmis-lab/biobert-base-cased-v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/hadarpur/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/hadarpur/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/hadarpur/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/hadarpur/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: tokenize and align labels (BIO scheme)\n",
    "print(\"Loading tokenizer:\", MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fb9ff",
   "metadata": {},
   "source": [
    "## Tokenization + label alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3952b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to align labels for tokenized inputs\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the list of token sequences and align the BIO labels\n",
    "    to the resulting wordpieces.\n",
    "\n",
    "    examples[\"tokens\"]: List[List[str]]\n",
    "    examples[\"tags\"]:   List[List[int]]  (indices into label_list)\n",
    "    \"\"\"\n",
    "        \n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,  # because we already have word tokens\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    \n",
    "    all_labels = examples['tags']\n",
    "    new_labels = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        # word_ids maps each subtoken position to its originating word index\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                # Special tokens (CLS, SEP, padding later)\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                original_label_id = labels[word_id]\n",
    "\n",
    "                if word_id != previous_word_id:\n",
    "                    # First subtoken of the word: use original label\n",
    "                    label_ids.append(original_label_id)\n",
    "                else:\n",
    "                    # Subsequent subtokens of the same word:\n",
    "                    # convert B-* to I-* to respect BIO scheme\n",
    "                    if original_label_id == label2id[\"B-Chemical\"]:\n",
    "                        label_ids.append(label2id[\"I-Chemical\"])\n",
    "                    elif original_label_id == label2id[\"B-Disease\"]:\n",
    "                        label_ids.append(label2id[\"I-Disease\"])\n",
    "                    else:\n",
    "                        # For I-* or O, keep same\n",
    "                        label_ids.append(original_label_id)\n",
    "\n",
    "                previous_word_id = word_id\n",
    "\n",
    "        new_labels.append(label_ids)\n",
    "\n",
    "    tokenized[\"labels\"] = new_labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bce7b724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and aligning labels...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5228\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5865\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tokenizing and aligning labels...\")\n",
    "remove_columns = tner_dataset[\"train\"].column_names  # [\"tokens\", \"tags\"]\n",
    "tokenized_datasets = tner_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns,\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f28d8c",
   "metadata": {},
   "source": [
    "## Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25ac1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6f852",
   "metadata": {},
   "source": [
    "## Load BioBERT token-classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20d35f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/hadarpur/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-Chemical\",\n",
      "    \"2\": \"B-Disease\",\n",
      "    \"3\": \"I-Disease\",\n",
      "    \"4\": \"I-Chemical\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-Chemical\": 1,\n",
      "    \"B-Disease\": 2,\n",
      "    \"I-Chemical\": 4,\n",
      "    \"I-Disease\": 3,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: dmis-lab/biobert-base-cased-v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /Users/hadarpur/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model:\", MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e1056",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd4348fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    p is an EvalPrediction with:\n",
    "    - p.predictions: np.array (batch, seq_len, num_labels)\n",
    "    - p.label_ids:   np.array (batch, seq_len)\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels: List[List[str]] = []\n",
    "    true_predictions: List[List[str]] = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        # filter out positions where label == -100\n",
    "        valid_indices = label_seq != -100\n",
    "        pred_seq = pred_seq[valid_indices]\n",
    "        label_seq = label_seq[valid_indices]\n",
    "\n",
    "        true_labels.append([id2label[int(l)] for l in label_seq])\n",
    "        true_predictions.append([id2label[int(p_i)] for p_i in pred_seq])\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4a42e",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5af98b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./biobert_bc5cdr_ner\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e6077",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bd0c604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6v/cz5ftvns42n726qk3351fkzr0000gn/T/ipykernel_66047/720657926.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Safetensors PR exists\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cdf016",
   "metadata": {},
   "source": [
    "## Training + evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32dc6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on validation and test\n",
    "    print(\"Validation metrics:\")\n",
    "    val_metrics = trainer.evaluate(tokenized_datasets[\"validation\"])\n",
    "    print(val_metrics)\n",
    "\n",
    "    print(\"Test metrics:\")\n",
    "    test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    print(test_metrics)\n",
    "\n",
    "    # Optional: detailed report on test set\n",
    "    print(\"Detailed seqeval report on test set:\")\n",
    "    predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        valid_indices = label_seq != -100\n",
    "        pred_seq = pred_seq[valid_indices]\n",
    "        label_seq = label_seq[valid_indices]\n",
    "\n",
    "        true_labels.append([id2label[int(l)] for l in label_seq])\n",
    "        true_predictions.append([id2label[int(p_i)] for p_i in pred_seq])\n",
    "\n",
    "    print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b09553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5,228\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,270\n",
      "  Number of trainable parameters = 107,723,525\n",
      "/Users/hadarpur/Desktop/biomedical-named-entity-recognition/env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  85/3270 00:16 < 10:26, 5.08 it/s, Epoch 0.13/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8c2a4",
   "metadata": {},
   "source": [
    "## Inference helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8931759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_inference(text: str, max_length: int = 256):\n",
    "    \"\"\"\n",
    "    Run NER on a new biomedical sentence/abstract.\n",
    "    Returns entities with type and char spans.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize as raw text (not pre-split)\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits  # (1, seq_len, num_labels)\n",
    "        pred_ids = torch.argmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    # Map subtokens back to words using tokenizer\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
    "    # We will reconstruct entity spans in a simple way: group consecutive non-\"O\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    # skip [CLS] (0) and stop at [SEP]\n",
    "    for i, (token, label_id) in enumerate(zip(tokens, pred_ids)):\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "            if token == tokenizer.sep_token:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        label = id2label[int(label_id)]\n",
    "        if label == \"O\":\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "            continue\n",
    "\n",
    "        # label is B-* or I-*\n",
    "        label_type = label.split(\"-\", 1)[1]\n",
    "\n",
    "        # Approximate char span via tokenizer offsets\n",
    "        offsets = encoded.token_to_chars(i)\n",
    "        if offsets is None:\n",
    "            # This can happen rarely; we just skip char span\n",
    "            start_char, end_char = None, None\n",
    "        else:\n",
    "            start_char, end_char = offsets.start, offsets.end\n",
    "\n",
    "        if current_entity is None:\n",
    "            current_entity = {\n",
    "                \"type\": label_type,\n",
    "                \"text\": text[start_char:end_char] if start_char is not None else token,\n",
    "                \"start\": start_char,\n",
    "                \"end\": end_char,\n",
    "            }\n",
    "        else:\n",
    "            # Same type? continue span\n",
    "            if current_entity[\"type\"] == label_type:\n",
    "                if start_char is not None and end_char is not None:\n",
    "                    # extend span\n",
    "                    current_entity[\"end\"] = end_char\n",
    "                    current_entity[\"text\"] = text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
    "            else:\n",
    "                # different type, close previous and start new\n",
    "                entities.append(current_entity)\n",
    "                current_entity = {\n",
    "                    \"type\": label_type,\n",
    "                    \"text\": text[start_char:end_char] if start_char is not None else token,\n",
    "                    \"start\": start_char,\n",
    "                    \"end\": end_char,\n",
    "                }\n",
    "\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4009243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example inference:\n",
      "Text: Paracetamol can cause liver toxicity in high doses.\n",
      "{'type': 'Chemical', 'text': 'Paracetamol', 'start': 0, 'end': 11}\n",
      "{'type': 'Disease', 'text': 'liver toxicity', 'start': 22, 'end': 36}\n"
     ]
    }
   ],
   "source": [
    "# Example inference after training:\n",
    "example = \"Paracetamol can cause liver toxicity in high doses.\"\n",
    "ents = ner_inference(example)\n",
    "print(\"\\nExample inference:\")\n",
    "print(\"Text:\", example)\n",
    "for e in ents:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
